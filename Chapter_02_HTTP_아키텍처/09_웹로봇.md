# 9. 웹로봇

## 9.1 크롤러와 크롤링
- 크롤러? 웹페이지를 한 개 가져오고 그 페이지가 가리키는 모든 페이지를 가져오고.. 를 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- 루트집합: 크롤러가 방문을 시작하는 URL 초기집합
  - 크고 인기있는 웹사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려있지 않은 페이지 목록으로 구성된 것이 좋음
- 검색한 각 페이지 안에 들어있는 url 링들을 파싱해서 크롤링할 페이지들의 목록에 추가
- 연결될 페이지를 탐색하다가 순환에 빠지게 됨 -> 어디를 방문했는지 알아야 한다. 순환에 빠지면:
  - 크롤러가 루프에 빠지면 같은 페이지를 반복해서 가져오는데 시간 허비. 네트워크 대역폭 차지.
  - 같은 페이지를 가져오면서 웹 서버 부담. 일반 사용자의 접근도 어려울 수 있음
  - 중복 콘텐츠가 넘쳐날 것
### 어떤 url이 방문했던 곳인지 관리하기 위해:
  - 트리와 해시테이블 사용
  - 느슨한 자료구조 사용(예. 존재 비트 배열-url이 크롤링 되면 존재비트를 만들고 비트 확인)
  - 체크포인트: 프로그램이 갑작스러게 중단될 경우를 대비헤 url 목록 디스크에 저장
  - 파티셔닝: 분리된 한 대 로봇들이 동시에 일하는 농장을 형성함
- url을 표준형식으로 정규화함으로써 중복이나 다른 url이라도 같은 리소스를 가리키는 것 방지
  - 포트번호 명시
  - 이스케이핑 문자 변환
  - 태그 제거
- 너비우선 크롤링
  - 방문할 url을 전체에 걸쳐 너비 우선으로 스케줄링
- 스로틀링
  - 일정시간동안 가져올 수 있는 페이지 숫자를 제한
- url 크기 제한
  - 일정 길이를 넘는 url 크롤링 거부
  - 가져오지 못하는 콘텐츠들이 있을 수 있지만 에러로그를 참고해서 어떤 일이 벌어지는지 감시에는 좋을 수 있다
- url/사이트 블랙 리스트
  - 순환을 만들어내거나 함정으로 알려진 사이트나 url 목록을 만들어 관리하여 피함
  - 문제를 일으키는 url을 만날때마다 블랙리스트에 추가
- 콘텐츠 지문
  - 페이지에서 몇 바이트를 얻어내어 체크섬(페이지의 간단한 표현)을 계산. 이전에 본 체크섬을 가진 페이지를 가져오면 크롤링 하지 않음.
  - 두 페이지가 다른 내용이어도 체크섬이 같을 확률이 적은 함수를 사용해야 한다.(MD5 같은 메시지 요약 함수)
  - 페이지가 동적으로 수정되거나 임의로 커스터마이징(날짜추가, 카운터 접근 등)등의 서버의 동적 동작은 중복감지를 방해할 수 있다.

## 9.2 로봇의 HTTP
- 로봇의 정보로 ```User-Agent```, ```From```, ```Accept```, ```Referer```요청 헤더를 보낸다.
- ```Host``` 를 사용하여 url에 대해 잘못된 콘텐츠를 찾는 것을 방지.
  - 예. www.joe.com, www.foo.com 두 사이트를 운영하며 기본적으로 joe.com를 운영하는 서버에 Host헤더 없이 joe.com, foo.com요청을 보내면 둘다 joe.com의 콘텐츠를 얻는다. foo요청이 foo에서 온 것이라고 착각할 수 있음.
- 시간이나 엔터티 태그를 비교하여 업데이트된 것이 있는지 확인한 후 변경이 있을 때만 콘텐츠를 얻도록 조건부 HTTP요청을 구현
- 로봇은 200이나 404같은 상태코드를 이해해야하고 이해할 수 없는 코드도 속한 분류에 근거하여 다루어야 함
- ```http-equiv```같은 메타 html 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보를 확인. 헤더를 덮어쓰기 위한 수단.

## 9.3 부적절하게 동작하는 로봇
- HTTP요청을 빠르게 만들어 내 웹서버에 과부하를 유발할 수 있음(폭주)
- url목록을 방문하는데 오래된 목록이라면 존재하지 않는 url요청을 많이 보낼 수 있다. -> 에러로그만 가득
- 길고 잘못된 url로 웹 서버의 처리능력에 영향을 주고 접근 로그를 복잡하게 만들 수 있다.
- 민감한 데이터를 가져올 수 있다.

## 9.4 로봇 차단하기
### robots.txt
- 웹 서버의 문서 루트에 ```robots.txt``` 파일 제공
  - 로봇이 접근할 수 있는 서버의 부분 정보를 담음
  - GET메서드로 robots.txt를 가져오고 파싱해서 접근허용된 url을 확인하고 요청을 진행
  - 리소스가 존재하지 않는다는 응답을 받으면 제약없이 사이트에 접근
  - 응답 상태코드에 따라야 한다(401, 403, 3xx..)
- robots.txt 파일 포맷
  - 빈 줄이나 파일 끝 문자로 끝난다.
  - ```User-Agent: <로봇 이름>```: 크롤링 허용할 로봇 이름. *로 모든 접근 허용할 수 있음. 대소문자 구분하지 않음.
  - ```Disallow```, ```allow```: 크롤링 허용, 비허용 url경로. 접두매칭(/tmp가 비허용이면 /tmpfile.html 비허용) 대소문자 구분.  
  - 명세가 발전하면서 다른 필드가 포함될 수 있지만 로봇이 이해하지 못하는 필드는 무시.
  - 한줄을 여러줄로 나누어 적지 않는다.
  - 주석 허용
- robots.txt 파일을 가져오고 캐시해야한다. 만료될 때까지 사용(Cache-control, Expires 헤더)

### 로봇제어 HTML meta 태그
- HTML 문서에 직접 로봇 제어 태그 추가
- ```<META NAME="ROBOTS" CONTENT="NOINDEX">```: 이 페이지를 처리하지 말고 무시해라. 색인이나 데이터베이스에 포함시키지 말 것
- ```<META NAME="ROBOTS" CONTENT="NOFOLLOW">```: 이 페이지가 링크한 페이지를 크롤링 하지 말라. 
- ```<META NAME="ROBOTS" CONTENT="INDEX">```: 인덱싱 허용
- ```<META NAME="ROBOTS" CONTENT="FOLLOW">```: 링크된 페이지 크롤링 허용
- ```<META NAME="ROBOTS" CONTENT="ALL">```: INDEX, FOLLOW 다 허용
- ```<META NAME="ROBOTS" CONTENT="NONE">```: NOINDEX, NOFOLLOW
- ```<META NAME="DECRIPTION" CONTENT="...">```: 웹페이지의 짧은 요약(검색엔진) 
- ```<META NAME="KEYWORDS" CONTENT="..., ..., ...">```: 키워드 검색(검색 엔진)
- ```<META NAME="RE-VISIT-AFTER" CONTENT="5 days">```: 쉽게 변경되는 페이지므로 n일 후에 다시 방문 지시


## 9.5 검색 엔진
- 검색엔진이 웹 로봇을 가장 광범위하게 사용
- 수십억개 페이지를 검색하기 위해 복잡한 크롤러 사용
- 대규모 크롤러가 요청을 병렬로 수행
- 풀 텍스트 색인(full-text indexes)라는 복잡한 로컬 데이터 베이스 생성
  - 웹의 모든 문서에 대한 일종의 카탈로그
  - 웹페이지를 수집하여 이 색인에 추가
  - 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터 베이스
  - ```사용자 -> 웹 검색 게이트웨이 -> 풀텍스트 색인 데이터베이스 <-> 검색엔진 크롤러/색인기 <-> 웹서버```
    - html 폼에 검색어를 넣어서 폼을 GET이나 POST 요청으로 게이트웨이로 보낸다.
    - 게이트웨이에서 검색질의를 추출하고 웹 서버에 문서의 검색결과를 주고
    - 웹 서버가 결과를 사용자를 위한 html로 변환한다.
    - 많은 페이지가 결과에 올 수 있으므로 검색어와 가장 관련이 많은 순서대로 정렬. (관련도 랭킹)
  
